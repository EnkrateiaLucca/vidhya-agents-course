{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425fb020-e864-40ce-a31f-8da40c73d14b",
   "metadata": {},
   "source": [
    "# Agentic RAG\n",
    "\n",
    "[Retrieval Agents](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/#agents) are useful when we want to make decisions about whether to retrieve from an index.\n",
    "\n",
    "To implement a retrieval agent, we simple need to give an LLM access to a retriever tool.\n",
    "\n",
    "We can incorporate this into [LangGraph](https://langchain-ai.github.io/langgraph/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50de5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0be6ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './assets-resources/pdfs/human-agent-collab-problem-solving.pdf'\n",
    "\n",
    "docs = PyPDFLoader(file_path).load_and_split()\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5787808e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 0}, page_content='Large Language Model-based Human-Agent Collaboration\\nfor Complex Task Solving\\nXueyang Feng1,2∗, Zhi-Yuan Chen1,2∗, Yujia Qin3, Yankai Lin1,2†\\nXu Chen1,2†, Zhiyuan Liu3, Ji-Rong Wen1,2\\n1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\n2Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\\n3Department of Computer Science and Technology, Tsinghua University, Beijing, China\\n{xueyangfeng, zhiyuanc2001, yankailin, xu.chen}@ruc.edu.cn\\nAbstract\\nIn recent developments within the research\\ncommunity, the integration of Large Language\\nModels (LLMs) in creating fully autonomous\\nagents has garnered significant interest. De-\\nspite this, LLM-based agents frequently demon-\\nstrate notable shortcomings in adjusting to dy-\\nnamic environments and fully grasping hu-\\nman needs. In this work, we introduce the\\nproblem of LLM-based human-agent collab-\\noration for complex task-solving, exploring\\ntheir synergistic potential. In addition, we pro-\\npose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC . This\\napproach includes a policy model designed to\\ndetermine the most opportune stages for hu-\\nman intervention within the task-solving pro-\\ncess. We construct a human-agent collabo-\\nration dataset to train this policy model in\\nan offline reinforcement learning environment.\\nOur validation tests confirm the model’s ef-\\nfectiveness. The results demonstrate that the\\nsynergistic efforts of humans and LLM-based\\nagents significantly improve performance in\\ncomplex tasks, primarily through well-planned,\\nlimited human intervention. Datasets and code\\nare available at: https://github.com/\\nXueyangFeng/ReHAC .\\n1 Introduction\\nIn today’s increasingly complex world, humans are\\nconfronted with multifaceted tasks stemming from\\ntechnical, social, and economic domains. Solv-\\ning these complex tasks necessitates not only hu-\\nman interaction with the environment but also in-\\ntricate decision-making processes. To alleviate\\nhuman workload and enhance the automation of\\ntasks in both professional and personal spheres, re-\\nsearchers have been actively developing advanced\\ntools for human assistance (Zawacki-Richter et al.,\\n∗Equal Contribution. The order is determined by dice\\nrolling.\\n†Corresponding Authors.\\nAgent Human Env\\nAllocator EnvEnv\\n(a)\\nAct Act(b)\\n(c)Agent\\nHumanAct\\nAct\\nI need  helpFigure 1: Different Levels of Automation. (a) No au-\\ntomation: Tasks are entirely performed by humans. (b)\\nFull automation: Tasks are completely executed by\\nagents without human intervention. (c) Conditional\\nautomation: Humans are required only for specific sub-\\ntasks, without continuous monitoring.\\n2019; Amershi et al., 2019). Recently, the emer-\\ngence of Large Language Models (LLMs) such\\nas LLaMA (Touvron et al., 2023), Gemini (Team\\net al., 2023) and GPT (Brown et al., 2020; Achiam\\net al., 2023) has marked a significant milestone.\\nLLMs’ remarkable abilities in task understanding,\\nplanning, and reasoning (Zhao et al., 2023b) have\\ngiven rise to the development of LLM-based au-\\ntonomous agents (Wang et al., 2023a; Yao et al.,\\n2022; Shinn et al., 2023). These agents are de-\\nsigned to leverage the LLMs’ capabilities to assist\\nhumans in solving complex tasks autonomously.\\nThe LLMs’ capabilities enable them to effectively\\nnavigate and address the complexities encountered\\nin real-world scenarios, thereby offering substan-\\ntial support in human decision-making processes\\nof task-solving.\\nDespite the remarkable progress of LLM-based\\nagents, there remains a notable gap in their intelli-\\ngence level to handle complex and dynamic real-\\nworld tasks with human-like proficiency. This limi-\\ntation poses a significant challenge to their practi-\\ncality in real-world applications, especially in sce-arXiv:2402.12914v1  [cs.CL]  20 Feb 2024'),\n",
       " Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 1}, page_content='narios where high accuracy is crucial, such as the\\nlegal or financial domains. Addressing this chal-\\nlenge extends beyond just enhancing the agents’\\ncapabilities. Incorporating human intuition and\\nwisdom is equally vital for the effective manage-\\nment of these intricate and evolving tasks, offering\\na complementary approach to the limitations of\\ncurrent agent technologies.\\nIn this work, we introduce the problem of LLM-\\nbased human-agent collaboration for complex\\ntask solving , aiming to augment the capabilities of\\nLLM-based agents by integrating human intuition\\nand wisdom. The idea is analogous to the evolu-\\ntion in autonomous driving technology, which has\\nbeen categorized into varying levels of autonomy,\\nranging from no automation, conditional automa-\\ntion to full automation (Khan et al., 2022; SAE\\nInternational, 2021). Referring to this framework,\\nwe define the different levels of human-agent col-\\nlaboration, as illustrated in Figure 1. Applying\\nthis conditional automation mode to LLM-based\\nagents offers a practical path for their deployment\\nin real-world scenarios, acknowledging the current\\nlimitations in their cognitive capabilities. Instead\\nof aiming for full automation, human-agent col-\\nlaboration under the paradigm of conditional au-\\ntomation enables humans to intervene the complex\\ntask-solving when necessary, while agents handle\\nmost of the sub-tasks. This takes advantage of both\\nhuman and machine intelligence.\\nWhile advancements in LLMs significantly en-\\nhance the capacity for mutual understanding in\\nhuman-agent collaboration, several crucial chal-\\nlenges persist. These challenges include defining\\nthe division of labor between humans and agents,\\ndetermining the granularity of tool execution, man-\\naging proactive interruption, and implementing\\nmulti-level intervention. However, our research\\nspecifically focuses on scenarios where humans\\ndirectly replace agents in action. The key chal-\\nlenge we aim to address in human-agent collab-\\noration lies in determining the optimal stages for\\nhuman intervention in task-solving and minimiz-\\ning such intervention to enhance efficiency. Some\\nresearchers have made preliminary attempts, by de-\\nsigning heuristic rules or specialized prompts to\\ndetermine the stages at which agents should seek\\nhuman assistance (Cai et al., 2023; Wu et al., 2022a;\\nMehta et al., 2023; Wang et al., 2023b). However,\\nthese rule-based or prompt-driven approaches are\\nheavily reliant on specific application contexts andlack universality. They often demand a deep under-\\nstanding of the domain and substantial experience\\nfrom the designers, otherwise, suboptimal design\\nchoices can lead to reduced performance. Apart\\nfrom that, a standardized formal framework and\\nuniversally accepted paradigm for leveraging large\\nlanguage models (LLMs) in human-agent collabo-\\nration is still lacking.\\nTo overcome the aforementioned challenges, we\\npropose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC , aimed at\\neffectively combining human intervention with the\\nautomation capabilities of LLM-based agents. Our\\nmethod, leveraging reinforcement learning, trains\\na policy model to dynamically identify the most\\nadvantageous moments for human input during the\\ntask-solving process. ReHAC is a learnable gen-\\neral framework that can be applied to various sce-\\nnarios and does not require additional prior knowl-\\nedge to design rules and prompts. For training\\nthis policy model, we collect a dataset compris-\\ning tasks collaboratively completed by humans and\\nLLM-based agents, utilized for the offline training\\nof the policy model. We conducted extensive ex-\\nperiments on three multi-step reasoning datasets:\\nHotpotQA, StrategyQA, and InterCode, using two\\npopular LLM-based agent frameworks, ReAct and\\n\"Try-again\". The experimental results indicate that\\nwith a policy model learned from limited data, Re-\\nHAC can effectively allocate human intervention\\nin human-agent collaboration scenarios, thereby'),\n",
       " Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 1}, page_content='\"Try-again\". The experimental results indicate that\\nwith a policy model learned from limited data, Re-\\nHAC can effectively allocate human intervention\\nin human-agent collaboration scenarios, thereby\\nachieving a balance between effectiveness and effi-\\nciency.\\n2 Approach\\nIn this section, we first formulate the problem\\nof human-agent collaboration for complex task\\nsolving, and then introduce our proposed ReHAC\\nmethod in detail.\\n2.1 Preliminary and Problem Formulation\\nComplex task-solving, inherently necessitating\\nmulti-step planning and reasoning, is convention-\\nally formalized as a multi-step decision-making\\nproblem. Historically, complex task-solving was\\npredominantly achieved through human-driven\\nmethods . These methods leveraged human cogni-\\ntive capabilities to determine the suitable action in\\neach step. Formally, considering a complex task\\nq, it is traditionally solved via a sequence of ac-\\ntions (a1, a2,···an), with each action determined'),\n",
       " Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 2}, page_content='by human decision-making, expressed as:\\nat=Human (q, st), (1)\\nwhere st= (a1, o1,···, at−1, ot−1)denotes the\\nhistory information of task state at step tandotis\\nthe observation after at−1is proceeded.\\nThe advent of LLMs has brought a paradigm\\nshift in this arena. Their impressive understand-\\ning and reasoning abilities have prompted research\\ninto LLM-based agents for complex task-solving,\\nthereby enhancing the level of automation in task-\\nsolving. These agent-driven methods (e.g., Re-\\nAct (Yao et al., 2022)), leverage LLM-based agents\\nto supplant human decision-making. This shift is\\nrepresented as:\\nat=Agent (q, st). (2)\\nThis evolution of such AI-driven techniques pro-\\nvides a way to the automation of complex task-\\nsolving.\\nHowever, limited by the current intelligence\\nlevel of LLMs, full automation based on agent-\\ndriven methods is not yet feasible in practical sce-\\nnarios (Kiseleva et al., 2022; Mehta et al., 2023).\\nInspired by autonomous driving (Cui et al., 2024;\\nFu et al., 2024; Bastola et al., 2024), we propose\\nthe problem of LLM-based human-agent collab-\\noration for complex task solving and explore the\\ndynamics and efficacy of the human-agent collab-\\norative methods for complex task solving. We\\nfirst explore a specific form of human-agent col-\\nlaboration: humans intervene in the complex task-\\nsolving process when necessary. Formally, we need\\nto determine whether a human or an agent makes\\ndecisions based on the actions’ complexity and\\ncontextual changes, i.e.,\\nat=Human (q, st)or Agent (q, st),(3)\\nIt is generally perceived that direct human in-\\ntervention in decision-making, particularly in real-\\nworld scenarios, incurs higher costs and diminishes\\nthe system’s automation level (Cai et al., 2023;\\nWang et al., 2023b). On the other hand, human\\nintervention plays an important role in enhancing\\ntask performance and flexibility. Therefore, the\\nobjective of human-agent collaboration is to en-\\nhance the effectiveness of complex task-solving\\nwith minimal reliance on human decision-making.\\nOne key challenge is to determine the stages in\\nthe task-solving process where human interven-\\ntion is most beneficial and effective, aligningwith the goal of minimizing human involvement\\nwhile maximizing task performance .\\n2.2 ReHAC\\nIn this work, we propose a Reinforcement learning-\\nbased Human-Agent Collaboration method, Re-\\nHAC. It formulates the human-agent collabo-\\nration problem as a Markov Decision Process\\n(MDP) framework, represented by the tuple\\n(S,A, P, R, γ ), where Sis the set of states, Ais\\nthe set of actions, P:S×A× Sis the state transi-\\ntion probabilities, Rserves as the reward function,\\nandγthe discount factor.\\nFor each action at∈ A, we define it as a tuple\\n(acollab\\nt, atask\\nt), where acollab\\nt indicates the subtask\\nis allocated to an agent or a human, and atask\\ntis the\\ntask action determined by agent or human:\\nacollab\\nt∼πcollab\\nθ1(acollab\\nt|st)\\natask\\nt∼(\\nπtask\\nθ2(atask\\nt|st),ifacollab\\nt = 0;\\nπtask\\nHuman (atask\\nt|st),otherwise ,\\n(4)\\nwhere πcollab\\nθ1is the collaboration policy model,\\nπtask\\nθ2is the agent-based task policy model, and\\nπtask\\nHuman is the human task policy.\\nTo balance the maximization of task perfor-\\nmance and the cost of human intervention, we de-\\nfine the reward function as:\\nR(s, a) =T(s, a)−λC(s, a), (5)\\nwhere T(s, a)is the measure of expected task re-\\nwards received after taking action ain state s,\\nC(s, a)is the number of human interventions in\\nthe trajectory after taking action a,λis a hyper-\\nparameter that serves as a penalty coefficient of the\\nnumber of human interventions. We utilize Monte-\\nCarlo estimation to compute this reward function.\\nOptimization: Following the REINFORCE algo-\\nrithm (Williams, 1992), we optimize the expected\\nreward:\\nJ(πθ) =Eπθ[R(s, a)], (6)\\nwhich aims to find an optimal policy πθthat ensures\\nthe maximization of task rewards while minimizing\\nthe human intervention costs, and θ= [θ1, θ2].\\nWe utilize the advantage function to enhance the\\nstability of optimization and important sampling'),\n",
       " Document(metadata={'source': './assets-resources/pdfs/human-agent-collab-problem-solving.pdf', 'page': 3}, page_content='for offline learning:\\nA(s, a) =R(s, a)−1\\n|A|X\\na′∈AR(s, a′)\\n∇θJ(πθ) =X\\nsX\\naw(s, a)∇θlogπθ(a|s)A(s, a),\\nw(h, a) =Clip\\x12πθ(s, a)\\nπbeh(s, a)\\x13\\n, (7)\\nwhere A(s, a)is the advantage function, the clip\\nfunction limits the importance sampling term to the\\ninterval 1−ϵto1 +ϵ, and the behavior policy πbeh\\nrepresents the policy under of the offline training.\\nMoreover, we have incorporated an entropy regu-\\nlarization term. This term encourages the policy\\nto explore a variety of actions, thereby preventing\\nthe policy from becoming too deterministic and\\noverfitting to the training data. Finally, the gradient\\nof objective function is as follows:\\n∇θ˜J(πθ) =∇θJ(πθ) +α∇θH(πθ(·|s)).(8)\\n3 Experiments\\n3.1 Experimental Setup\\nDatasets Following Yao et al. (2022); Shinn et al.\\n(2023); Liu et al. (2023b); Xu et al. (2023), we eval-\\nuate the efficacy of our method on question answer-\\ning and coding datasets: (1) HotpotQA (Yang et al.,\\n2018) is a Wikipedia-based question answering\\nbenchmark which needs model to perform multi-\\nhop reasoning over complex questions. (2) Strate-\\ngyQA (Geva et al., 2021) is a question answering\\nbenchmark with questions that need implicit rea-\\nsoning. (3) InterCode (Yang et al., 2023) is an\\ninteractive coding dataset that enables agents to\\nreceive feedback from the code interpreter. In this\\nwork, we use InterCode-SQL part, which requires\\nmodels to write SQL statements to fulfil the query.\\nImplementation details We use LLaMA-2 (Tou-\\nvron et al., 2023) as the collaboration policy model\\nπcollab\\nθ1and use Low-Rank Adaptation (LoRA, Hu\\net al. (2021)) methods to train the policy model.\\nIn all experiments, we utilized ChatGPT (gpt-3.5-\\nturbo-0613) to simulate the agent policy πtask\\nθ2.\\nMore model implementation and data collection\\ndetails can be found in Appendix A.1.\\nIn this study, we set humans and agents to solve\\ntasks under the ReAct framework (Yao et al., 2022)\\nfor question-answering datasets. The action space\\nofataskis {Search[entity], Lookup[keyword], and\\nFinish[answer]}. All actions are supported by aWikipedia web API, following the original Re-\\nAct implementation. For the InterCode dataset,\\nwe solve tasks under the “Try Again” framework\\n(Yang et al., 2023). Here, agents and humans in-\\nteract with the code interpreter through the action\\natand receive execution outputs from the code\\ninterpreter as observations ot. The task-solving\\nprocess ends if any one of the following conditions\\nis satisfied: 1) the Finish[answer] action is exe-\\ncuted actively by πtask\\nθ2for the question answering\\ndataset. 2) the task reward T(s, a) = 1 for Inter-\\nCode dataset. 3) the number of actions texceeds a\\npre-defined step threshold.\\nReward Calculation For all datasets, the final\\nreward is computed as equation (5). For question\\nanswering datasets, we choose the F1 score as the\\ntask reward T(s, a). For the InterCode dataset,\\nfollowing Yang et al. (2023), we use Intersection\\nover Union as the task reward T(s, a).\\nBaselines We compare our method ReHAC with\\nthe following baselines: 1) Agent-only which car-\\nries out all actions by agents. 2) Human-only,\\nwhich conducts all actions by humans. 3) Ran-\\ndom, which selects an agent or human randomly\\nat a probability of 50% to perform each action. 4)\\nPrompt, which prompts the agent to actively decide\\nwhether the action is executed by itself or a human.\\n5) Imitation Learning (IL), which trains the pol-\\nicy model to decide whether the action should be\\nfinished by an agent or human by the IL method.\\nMore details about baselines can be found in the\\nAppendix A.2.\\n3.2 Overall Results\\nIn this section, we verify the effectiveness of our\\nproposed ReHAC method for human-agent collab-\\noration on the HotpotQA dataset.\\nHuman-Agent Experiments Figure 2(a) shows\\nthe evaluation results of human-agent collabora-\\ntion on the HotpotQA dataset. From the figure,\\nwe can observe that all human-agent collabora-\\ntion methods outperform Human-only and Agent-\\nonly methods. This underscores the importance')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "634a1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6478c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    'retrieve_info_from_paper',\n",
    "    'Search and return information about a paper.')\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "669e4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51d2a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72909b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: AgentState):\n",
    "    messages = state['messages']\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model='gpt-4o')\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    return {'messages': [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a312901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Graph initiliazation\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Nodes\n",
    "graph.add_node('agent', agent)\n",
    "retrieve_node = ToolNode([retriever_tool])\n",
    "graph.add_node('retrieve', retrieve_node)\n",
    "\n",
    "# Edges\n",
    "graph.add_edge(START, 'agent')\n",
    "graph.add_conditional_edges('agent', tools_condition, {'tools': 'retrieve',END: END})\n",
    "graph.add_edge('retrieve', 'agent')\n",
    "\n",
    "compiled_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44adf6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCADuAOUDASIAAhEBAxEB/8QAHQABAAIDAAMBAAAAAAAAAAAAAAYHBAUIAQIDCf/EAFgQAAEEAQIDAgcJCQoLCQEAAAEAAgMEBQYRBxIhEzEIFBUWIkFRI1VWYXGTlNHTMkJTVHWSlbTUCRc3OHKBgpGhsTQ2Q0RSYnN0srPBGCVXY2Sio8Ph8f/EABsBAQADAQEBAQAAAAAAAAAAAAABAgQDBQYH/8QAMxEBAAECAgcECQUBAAAAAAAAAAECEQMEEiExUZGh0RRSYXEFExUzQYGxwfAjQlPh8WL/2gAMAwEAAhEDEQA/AP1TREQEREBERARFi5PJV8PQnuWnlkELeZxa0ucfYGtHVzidgGjckkAbkqYiZm0DKWDazuNoyFlnI1K7x3tlna0/1ErTDAW9Tjt87JLXqu3MeHgl5Gtae7t3tO8j/a0HkG+w59uc51bRen6cYZBg8bE0ADZlSMd3d6l30cOnVVN58Ov55p1Pp51YT34ofSmfWnnVhPfih9KZ9a8+a2F96KH0Zn1J5rYX3oofRmfUn6PjyTqePOrCe/FD6Uz6086sJ78UPpTPrXnzWwvvRQ+jM+pPNbC+9FD6Mz6k/R8eRqePOrCe/FD6Uz6086sJ78UPpTPrXnzWwvvRQ+jM+pPNbC+9FD6Mz6k/R8eRqeWanw0jg1mWoucfULLCf71smua9oc0hzSNwQdwQtW7SmEewtdh8e5rhsQarCD/Ytc/QNCi502Bc7Ttrfm3oANgef/Mg+4cD6zsHd+zgTulsGdkzHnH59JRqSZFqMHmZLz56d2EVcpV27aIHdj2nukjPrYdj8YIIPULbrjVTNM2lAiIqgiIgIiICIiAiIgIiICIiAiIgKMZjbLa2w+Nfs6tTgkyUrD99IHNZD8oHNK7r3FrD6txJ1GJx4nxIqyv3DL+MfC123TnikDuXf2kSuI/kn2LRg7Zn42n6dEwk6L1kkZDG6SRzWRsBc5zjsAB3klQEeEJwscQBxL0eSe4DPVftFnQsBVlpzjvQ1fqLMYzDaY1LkauMsW6T8xFTjFKazX3EsLHulDubmBYC5rWl3c71rN/7QvCv/wAS9H/p6r9oq207o3VTuP8AV1JgNJv0ZpyxauTZ2/Bm4bFDUELo3CtM2qwktnLuzeZC1pA5gXP36huuBfHzNcQuElnVWd0bmoLNUTybUasUjL4bYlYI6sbJnvc5gY1rucN9LuJHVbCPwmsBDprWeUymA1Hgbuk6TMjkcLk6ccd013hxZJEBKY3h3I8dH9C0g7KusTw/4oYngBqLhpSwUuNyFCWZ1HOVcvDEzK135AzviiLXdpA98D3s5ngBpPf6xFcnwC1TZpcU26d4YQaPx+pNGNxdDGxZKq+V1yOV593LX8ofIJujg542i9JwJAQWdxF8JXMYXAaUyuB0JqF9TMaho45r71aux1utKeYmBjrDXNe8eiztQ3Y78wb0KvTD35MriadyWjZxktiFsrqVzk7aAkblj+Rzm8w7jyuI3HQlVjxz0VqDUOhtLS6ex8eUy+nM5jc0MW6dkBtNrvBfE2Rx5WuLSdi47dFt4uO2jsXFHW1ZqTT+jNQtaDbwWVztMWahI3a1+0hG5aWu6dNnBBYaKAu8IHhcxrC7iTpBoeOZpOdqjmG5G4909oI/mUq05qrC6xxjclgMxQzmOc4sFvG2WWIi4d452EjcesboNZq/bF5DB5pgDXw3I6Mzuu74bD2xcvzphd/R+NSdRnXrTapYrHtBMtvK1A3Yb9IpWzv+T0IXdVJlor93RM7dfD/brTsgREWdUREQEREBERAREQEREBERAREQFqtRYU5mlH2MjYL9WUWadhwJEUwBAJAIJaQ5zXAEbtc4bjfdbVFamqaJ0oGow+oYcpJJTnZ4llYR7vRkPpAd3OwkDnjPqeBt6js4FozvJtT8Vg+bH1LHzOn8dqCGOPIVI7PZkujedw+JxGxLHjZzDt03aQVqDoXk6V9Q52uzoAwXe12HyyNc4/KTuu1sKrXe3OOO3l806m/8m1PxWD5sfUsgAAAAbAepRfzIn+FOe+fi+yTzIn+FOe+fi+yT1eH3+UptG9KUUW8yJ/hTnvn4vslU3guZnUHGHhBR1NntT5VuSmuXIHCo+OOPlisPjbsCw9eVo3696erw+/yktG90EvhJRrTPL5K8T3nvc5gJKjvmRP8ACnPfPxfZJ5kT/CnPfPxfZJ6vD7/KS0b0g8m1PxWD5sfUvlfyOP07RdYtzQUarSBzOIaC49wA9biegA6k9y0o0RNsQ7U+eeD02NiMf2iMFZeL0Xi8XcbdEc1y+3fa3fnfYlbv38peTyD4m7BNHCp21X8o6/2jU+eIpWMvlhnL8BrBkboaFWTo+KN3KXSSD1Pdyjp960bd7nBSFEXKuua5Jm4iIqIEREBERAREQEREBERAREQEREBERAREQEREBc7+AR/FuxX5TyX65KuiFzv4BH8W7FflPJfrkqDohERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQFzv4BH8W7FflPJfrkq6IXO/gEfxbsV+U8l+uSoOiEREBERAREQEREBERAREQEREBERAREQEREBERAREQERa7PZuHAY82ZmPmc57YooIgC+aRx2axu/Tcn1nYAbkkAEi1NM1TFMbRsUUKfntXPdzMx2Fiae5j7kriP5xEN/wCpevlzWH4jg/pU32a1dlr3xxhNnK37p7wLOrdCY7iNjK5kyengKuQ5BuX0nuJa72ns5Her1SvJ6Bcufue3BWXipx6x+YnY4YXSTo8tYlG4BsNdvWj3HcS9vP8AG2JwX6fZ7zi1Pg8hh8nicDbxuQryVbNeSzNyyRPaWvafc/WCQq58HPgrk/Bu0RY09hYsTkHWrklyzfszyNllJ2DGnaPoGsDQB3b8x6cxCdlr3xxgs6LRQjy5rD8Rwf0qb7NeRnNYb9aWD2/3mb7NOy1744wWTZFH9P6mnv3HY7J1GUMmIzMxsMplimjBAc5jy1p6FwBBAI3HeDupAs1dFWHOjUgREVAREQEREBERAREQEREBERAREQEREBERAUP4gna7pEeo5c7gj/0lk/3gKYKHcQf8P0h+WD+p2Vqy3vY8p+krRtZyKHcXtTs0hw9yuSdm36fkaI4ob0NLx2Vsr5GsYyOD/KPcXBrW+1wJ6BUDS478QNJ6S4tQ5Rl+/ldNY+hkMdZz+Nr1rLI7LpGPfNFVeY3Mj7MydNiQHBwGy7zVEKur0XJmV43aq4b2OINuLWbOJ2Mw+n6FmnbjrVY4Ib1q0YWxyGANDthySAc7fQJB3OzhvMfq/jBp6LNT5KvnrOIZgr9mTI5/H4us6hbjhL4XQtqzyc7HEOBZI0kbNPMeqjSgdLr1ZIyUEsc14BLSWnfYg7Ef1rnnRusddVc5wmfmNWHMVde4yd9isMdBA2hO2kLLHwFreYjo5pEheDvuNu5Zvgc4LKY/hnLbuaku5arNk8pHHRsQV2RwPbkLAfI10cbXkvILiHOIBPogDYJFV5sLjkO3ELTm3rrXAfk2i/8AxTtQST+ELTf+73P7olO1TM/s8vvKZ+AiIsSBERAREQEREBERAREQEREBERAREQEREBQ7iD/h+kPywf1Oypiq846aswvD/RA1XncjBjaeFssth0+/uri10fZNABJe5sjgNgeu3q3K0ZeqKcSJnx5xZMbXpxA0HjuJGmJ8Hk5LMEL5YrEVmlL2c9eaKRskUsbtjs5r2tI3BHTqCOirrMcAY8Ph9Z5HG5bUuotRZ7EDHWvG8tHBJcLC4xkSdjywuAe5o5WhmzjuwkkqxKmu6l2nXtRY3NvhsRtljdHiLEgc1w3BDmsIO469Cvr551/erPfoS39mt/qK5/anRlRHCHg9qexFn9M6txF2nw1yGLdWlwmcs46ad9pz2+6ROoRRtYwMDvujzc3IQBsrNwPBCDD4jL423rDVmfqZDHSYsR5fIMmFeF7eUmMCNoLwO57w53xnqpV551/erPfoS39mtbp/ivgNWYyPJYPyjmcdI5zGW8fjLE8LnNcWuAexhBIIIPXoQQojL1x+2TRnc+NfhJh603D+Vtm8XaKhdBjgZGbStdVNY9t6HpHkO/o8vpfF0XroThJQ4dZvLXMTmMwcdkJp7PkOxYY+jWlmk7SR8LeQPaS7mOxeQOd2wG63XnnX96s9+hLf2aDWVckDyXnuvtwtv7NT6ivuyaM7n3k/hC03/u9z+6JTtVHw+4iae4i8T8xj6F0x5XSsRr2sZZjdFYD5hG7tOVw+4AaG9DvzOcCBs3mtxZMzMXpp3R95n7okREWNAiIgIiICIiAiIgIiICIiAiIgIiICLV6k1Th9H4w5HO5Snh6Ae2M2b07Yo+dx2a3mcQNyegCh1t+q9cao1jpTKYGTA6JOPFapqOjleS9amkYOd0LWDeIMDiOYnfmYNuYE8oZ2suJ3mnrPSOnIdO5nNz6gmkabePrc9ajEwDmlnkJAaAXM6d5BO3UAH10fw6yGLdqM6p1LZ1tHlcl47BUyNWJtahExwMMUUYHe3lYS4nq5ocA0kk7rQWh8Vw20fitM4SOaPFY2HsYG2J3zSbbkkl7iSdySfYO4AAADfoCIiDnXw6+Of7ynAzINoWTDqPUHNjMdyHZ8YcPdph6xyMJ2I7nPYuWP3LvjkMHqrK8MsnY5aeX5r+L53dG2mN91jH8uNvN8sXtcuu/CB8DzSHhJaixuX1TmdRVnY6r4rXqYy1DHXaC8uc/lfE887t2gnfqGN6dOvOPgVeBjonU+lNK8TrGU1BBqHHZqWxFDXtQtrONW24Rtc0xFxa4RgOAcN9ztsg/QRERBG9Y6MbqXBZyrj78+msvlKgqnO4tjG3Ig3m5CHkEnlLnbD1czti0ndR2jre5oXOaJ0NqCHN6jyuTpOZJqqDGBtKWzG3dzZezJELnhr3AEcoA7z12sZEHgEHuO68qlZtG5Twb9BZE8M9PZPXTrWa8emwuRzBL68MhHbCs6TfuILg0kkl7iS7uVs0dQY3IZK3jYL9WTKUmxut0Y52OmrB43Z2jAd27juJ7/AFINiiIgIiICIiAiIgIiIC+fjEX4Rn5wX0XPvhA8RMnwr4U5nUeHoHIZCsI2xtLGuZHzPDS94L27tG/qJO5HQjdBf3jEX4Rn5wTxiL8Iz84Kg81xmq6exWDkvabz8WdzUssNHTTIIZMhJ2e5e7ZkpiDQ3ZxcZNgHN32J2WEfCH00dNV8lHTy82RnyT8MzT0dQeUvHWDmfAYi7lBaz0y4u5A0g82xCDonxiL8Iz84KMZXiXg8VrjE6RklsPzeThksQiKpJJDGxgJ5pZQORm/KQAXAk9PWFz9qnjlJl8VgJMAb2ByTNY4zB5jGZSrG21AyaRpfG9p52gPjc0h7CdwfRd3qYU+MVDJ6+vaWx+EzmRdQstpXctWqtNGrOYxJyPeXh3Rrm7kMLRuBuglOA4Y39d6WrwcYotO6xydbLOylSvSpkVKWwLYmAPJMnKC7q8debYg8u5tIDYbDoFodK/51/R/6rfoCIiAiIgLnbwBv4tuJ/KWS/XJV0SudvAG/i24n8pZL9clQdEoiICIiAodl+GuKGdy+rcFjcbjdfW8ZJj4s7LW5zsdiztWgjtAHNZ3nfZoG+ymKIKuw/FU6DqaG07xTzGIoa81E6WtC3GMl8TszMcAGsc4eiXB8ewdtu4kN9QVorGuY2nkXV3W6sFp1aUTwmaMPMUgBAe3cei4AnqOvUqr7k2c4E4DWup8xmNRcRsVLeF6piKlGKS3joHu91ZGWlvaMZzF2x25Ws2A7yQtlFi4vIxZfHVLsLZWRWYWTsZPE6KRrXDcBzHAOaevUEAg7grKQEREBERAREQFQvHfReR4h8I9TafxPZeU7lYeLNmdysfIx7XtaT6ty3bf1bq+lr/INH8Cfz3fWg5k1DT1tl9SaM4hV9ESxZTDsu4+5pqXJ1jPJBO2IiWKUP7Ldr4h6LnDdpPd3KK0uE+uMdn6vEs4SCxqUaltZaXS0d6MFlOam2p2bZj7mZ2tY15O4adyOb29i+QaP4E/nu+tPINH8Cfz3fWg46zPCzXWfGf1s/BQ1tQW9TYfNVtMOvxl/i1ANaGOnHuYleOd3Qlo9EcxW+1BpDVmR4zYrOaf0hNpeUZCs7K6hiy8Rr5Og2P3SGeq07vlG/IxxaeXlBDwOivTStXU9nX2saWewVOtpas6scBkqs57S010e87ZWF5Icx+wB2aCPkUz8g0fwJ/Pd9aDA0r/nX9H/AKrfrHqUIKPP2LOTm236k93yrIQEREBEXN2vOJOoePuqshw34VZB+Nw1J/Yao11B1ZTH31Sm7ufYI6Fw6MB799iA+3EvixqPi1rC7wt4RXBWtVj2epdasHNBhWHvhhI6PtHYjYH0OvUEEsuDhfw0wnCHQ2M0pp+KWPG0GENdPIZJZXucXPke71uc4ucdth16ADYL24acM9O8I9H0tM6YoMx+LqjfbvkmefupJHd73u9ZPydAABKUBERAREQEREBERBV+jsLga/HriJlKmqJcnqGzTxsN7BvO7cZEyN5i5fYJOd79vaSfWrQVZaNyuEs8c+IlGnpSbG5utWxzr2oHtIjybXROMbWn19mN2n5VZqAiIgIiICIiAiIgIiIKv40Y/C4K1p7iTntT5LTmL0U+e1ZbTD5IbUczBEWSxNDuYbkbENJG52I7xn6Y8IHhxrbWjNJ6e1lic7nX0jkG18dOLDDEHcp91ZvHzg9THzc4HpcvL1XKn7ozwZ4p6jqT6o0zqHNZjRQqRxZXSlew5sVcRu5xMIGACZm+znF4c9rhvvyACOif3LrH+O+EfelLd/E8BZn39hMsMf8A9iD9YkREBeCdhuegXiSRsTHPe4MY0Euc47AD2lcw6i1Vm/C7zt3SOiL1jDcJ6Upr5/V1Y8suXcPu6dF3+h6ny9xB2G46SB9tW68z/hQalv6E4bZGbEaCoymvqXXNU+lOfvqVB3c5xHR0o6AHp025770JoPBcM9KY/TemsdFi8PRZyQ14h/W5xPVzidyXHckkkr76R0jhtB6boYDT+PhxWHoRiKvUrt2axv8AeSTuSTuSSSSSVuEBERAREQEREBERAWl1XrXT2hMdHkNS57GadoSSiBlrK3I6sTpCC4MDpHAFxDXHbv2afYt0q28Ing/V46cINQaSnDG2rMPa0J3/AORtM9KJ2/qHMOV233rnD1oIhp7wjtOfvm6t8r8UOH/mT2NTyF2WoKfb9pyHxntPdN9ubbbf1K78bkqmZx1W/QtQ3qFqJk9e1WkEkU0bgHNexwJDmkEEEdCCvww4O8EczxS41Yvh86CajcfedXyJcz0qccRPbucOoDmhruh73bD1r9y8JhqWnMNQxONrsqY6hXjq1q8f3MUTGhrGj4gAB/MgzUREBERAREQFh5fKV8Ji7eQtOLa1WJ00hA3PKBudvjWYorxRJGgcwQdj2Teo/ltXXCojExKaJ+MxCYi82YL72rr3u0d3F4pjuraslF9l8Y9jpBM0E92+zdgd+p716c2sff7E/oeT9oW4RejpRGymOEdE3afm1j7/AGJ/Q8n7Qqv0T4OMPDnitneIGnb+NxeXzVU1bdWDFOFTq9r3PbH2/ouc5jd9jt39NySrhyF+tiqFm7dsRVKdaJ009id4ZHFG0Euc5x6AAAkk9wC80rkGRpwW6srZ608bZYpWHdr2OG4cD7CCCmn4RwjoXa3m1j7/AGJ/Q8n7QnNrH3+xP6Hk/aF7W9VYujqXH6fmslmXvwTWa1fsnnnjiLBI7mA5RsZGdCQTv032K2yafhHCOhdW3FXhnqXi5pZ+ncprMY7EzyNNyHFY90DrkQ74ZH9sXCN33waWkjpvsSFIdPYDP6TwdLD4bIYLG4ulEIa9SthHsjiYO4ACx/8A1ShE0/COEdC7UnUed02wW8zYoZHGNIFiSrWdWkgaTsZNjI8Pa3cEt9E7cxBcQGmcqueJDizh7qZzSWuGNsEEHqPc3KxlnzFMaFNcRaZmY4W6k7LiIiwqiIiAvjcuQY+pNatTx1q0LDJLNM8NYxoG5c4noAB6yvsqO4u6qlzWpH4SGQtxuMLHTtHQTWS3mAPtaxrmEDu5nEkbsaRuyWVqzmLGHGqNsz4DZ57jnYmldHp7GMdAO69ki5gd8bYRs4j+U5h+L1rQu4uazdsfGcM0+sNx8m39s6iqL7jD9HZXDi0URPnrNLclH77Ws/xvD/o6T7ZP32tZ/jeH/R0n2yi6Lr2HLfxxwRpS0ulcFLo3ipqjiFjG4uPUeoo2RW5HUXmNgGxeY29ru0yFrXP6nctB6dd7C/fa1n+N4f8AR0n2yi612oNRY/S2PF7J2PFapmig7Tkc/wBOR7Y2DZoJ6uc0b9w369FE5PKxF5w44QnSlOf32tZ/jeH/AEdJ9svdnF7WUZBM2FlG/VrqEo3HxET9P7VFETsOV/jjgjSlbeleNNXI2IqmcqDEWJCGsssk7Ss5xOwBcQCwn/WG3q5t1Za5YexsjHMe0OY4bFrhuCPYrZ4L6smv17eAuymaei1staR53c+u7pyk+sscNt/Y5nedyvnfSXoyjBonHwNURtj7wnas5ERfLgorxS/xAzP+yH/G1SpRXil/iBmf9kP+Nq0Zb39HnH1Wp2ww9TzSV9NZaWJ7o5WVJnNew7OaQwkEH1FcqRZXU+jvBj0NqarqjN5XVGrhiMbayOVzEgiqx2HN3ezma9kLuUiPt+zc7d3MeYrrm3VivVZq07eeGZjo3t3I3aRsRuOvcVHjw00w/QEWiJcPBZ0rFUZRZjbJdKwQsADGkuJcSOUbOJ33AO+/VaZi6rnvUfD3iDiOGvFKDUN2zDpObSdySOo/VVnLW2XY2F7XNmfBE9sT2hzXxlzmu2222c4LM1Dg7umeDnBzGaf1RqDG+XNQ4mOzdGVmnn7GWq/tImPkc7aPZo2Z9wDsQ1XXo7g9pHQdPJ1cPiiyHJxthuC5amuOnjaHBsbnTPeeQB7wG77ekenVYeD4D6I05jcdQoYiWKnjsjFlacMmQsytgsRtc2NzOeQ7Na1xAZ9x1+5VdEVtqSO5wn4wYaPEZfUGVpt0hmbrsbk8zZuR2JoJIXxEtke7d3ur283fsQO4BRnhBhuLGrKehtaRZYy18qa93LWLGrZrNe1VlbvNHHQ8UbHA9oceURvBYWbFzupXS1vSOJvapoajnqc+ZoVpqdez2jxyRSljpG8oPKdzGzqQSNuhG5UX01wD0Fo/UrM9hsA2hkIpJJoRFan8XgfICHujgLzFGSHOBLWDvKaM3FO6P1PqfJamwnCqfL5Wxl9IZi3fzWQFqQWbuNgDZKDZJN93dv4zA1wJPN2EoO/VaXhCzi1xNw2luIFPJckuRux27Usuq5X0/FhORNWGN8U7NhDA9g2k5w4Al5O66pq6ZxdLUN/OwUoosvfghrWbbR6cscReY2n1dDI/4+o37htEsfwD0FidXectLANq5Xxl10GK1O2uLDgQ6UVw/sg87ndwZv1700ZG84lfwd6n/Jln/lOVjquOJX8Hep/yZZ/5TlY6nMe5o86vpSt8BEReeqIiIC5eyJc7UGoHPJMhy1zfcbd07w3/ANob/MuoVQfFTTkmntYT3msIx2YcJWPA6MsBuz4/6TWB49vun+ivo/QeJTTjVUTtmNXy/OSfgiqLXZ2LLTUeXC2qVO5zA9pfrPnj5fWOVsjDv3deb+ZR4Y/iF13zumT7P+5bH7WvsaqpibRTM8OrmyeKepLmj+HGpM1j4xJeo0ZZoQ5vMA4N6OI9YHefkVcaM09rylfxeWdkHSYiarJJffZ1FJkBZa6ElkkUbq7GxEP5T6BA2JG3crHxuM1bLbEecyWAv4p7XMnrVsTNE+QFpG3M+w9u25G4LTuNx6918dNcJdKaQuPtYnFmrK6J8A3szSNjjcQXMja55bGCQOjQO4LLXh14tcV7Ij4X/wB2/JKqdE2svisBwh1C/UWYyNvPWIqWQhvXXywzMfWleD2Z9FrmmNuzgOY9eYuJK02Wgu6z4V09c5LPZSa9dz1XfFstFtKuxuRZG2HsR6O7Q0EuPpcw79uiv2Dh/gKuMwGOioctPAysmx0fbSHsHtY5jTvzbu2a9w9InvWnt8ENE3cpPkJcIBYntNuyCO1NHGZ2uDxL2bXhgdzAEkDc9d99ys85XE0dG8Tq3zttEX5TxE5RQ+ahr8zPMWc022IuPI1+FsFwHqBPjY3P8wXp5P4h+/2mf0JY/a16HrJ7s8uqEzUo4Tl7eJVIsH3VCy1/T73miP8AeGqI0W2WUoG3JIpbYjaJpIIzHG5+3pFrS5xaN99gSdvaVaXBDTUhku6lnZysnj8Upbjq6IO3kkHxOcGge0Rg9Q4LH6RxacPK1zV8Yt85/LrU71tIiL85SLX5/Dx6gwl7GzPdGy1E6LtGfdMJHRw+MHY/zLYIrU1TTMVRtgQN97UFA9jY0xcyErOhs42at2Mn+sBLMxzd+/Yg7b7bu23Xr5bzPwLzfz1H9pU+RbO1f8Rz6rXjcgPlvM/AvN/PUf2lPLeZ+Beb+eo/tKnyJ2qO5HPqXjcgPlvM/AvN/PUf2lPLeZ+Beb+eo/tKnyJ2qO5HPqXjcgPlvM/AvN/PUf2lPLeZ+Beb+eo/tKnyJ2qO5HPqXjcr+xj8xq6u7GzYezhKE45bU92aEvMW/pMjbFI/0nDpuSAA4nqRymwERcMXFnEtFrRG7+0TIiIuCBERAWHl8RTz2NnoX4G2ak7eV8biR8YII6gg7EEEEEAgghZiKYmaZvG0UdnuDWexUrnYiaHNU/vY7DxDZaPZvtyP+X0PkPetC7Q2r2bb6UuuPr5bNQ7f/Muj0XvYfpvM0U2qiKvOJ+0wnVuc3eZOr/glf+k1Pt08ydX/AASv/San266RRdfbuY7lPPqatzm7zJ1f8Er/ANJqfbp5k6v+CV/6TU+3XSKJ7dzHcp59TVuc3eZOr/glf+k1Pt17s0JrCUgDS1tm523ltVQB8u0pP9QXRyKPbuY7lPPqatyodL8FLM8zJ9TTwGAdfJtNznNf/tJCASPa1oA9rnDcG3GMbGxrWtDWtGwaBsAF7IvHzOaxc1VpYs9IBERZEP/Z",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(compiled_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94478cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_GgtP493vqXxwhYKfIYkywKyf', 'function': {'arguments': '{\"query\":\"how do the authors set up the collaboration between the human and the LLMs?\"}', 'name': 'retrieve_info_from_paper'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40'}, id='run-9e8aab5e-c16a-457c-ad69-195ca283d046-0', tool_calls=[{'name': 'retrieve_info_from_paper', 'args': {'query': 'how do the authors set up the collaboration between the human and the LLMs?'}, 'id': 'call_GgtP493vqXxwhYKfIYkywKyf', 'type': 'tool_call'}])]}\n",
      "'\\n---\\n'\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='Wu et al., 2022a; Mehta et al., 2023). Further-\\nmore, there is an increasing emphasis on develop-\\ning specialized prompts that motivate LLM-based\\nagents to proactively seek human input, thus nurtur-\\ning a more interactive and collaborative dynamic\\nin these partnerships (Huang et al., 2022; Wang\\net al., 2023b). However, the effectiveness of these\\nmethods relies on designing high-quality rules or\\nprompts. This is highly dependent on the designer’s\\ndomain knowledge. Poor design may result in a sys-\\ntem that cannot accurately understand or respond to\\ncomplex task requirements. Our research focuses\\non designing a generalised and learnable method\\nthat coordinates human to effectively work with\\nLLM-based agents in the form of direct planning.\\n6 Conclusion\\nIn this paper, we propose the problem of large\\nlanguage model-based human-agent collaboration,\\ndelving into the synergy of human intuition and\\nexpertise with the computational prowess of LLM-\\nbased agents, particularly emphasizing their appli-\\ncation in intricate decision-making tasks. We intro-\\nduce a reinforcement learning-based approach for\\nhuman-agent collaboration, named ReHAC. Cen-\\ntral to ReHAC is a learnable policy model designed\\n\\nWu et al., 2022a; Mehta et al., 2023). Further-\\nmore, there is an increasing emphasis on develop-\\ning specialized prompts that motivate LLM-based\\nagents to proactively seek human input, thus nurtur-\\ning a more interactive and collaborative dynamic\\nin these partnerships (Huang et al., 2022; Wang\\net al., 2023b). However, the effectiveness of these\\nmethods relies on designing high-quality rules or\\nprompts. This is highly dependent on the designer’s\\ndomain knowledge. Poor design may result in a sys-\\ntem that cannot accurately understand or respond to\\ncomplex task requirements. Our research focuses\\non designing a generalised and learnable method\\nthat coordinates human to effectively work with\\nLLM-based agents in the form of direct planning.\\n6 Conclusion\\nIn this paper, we propose the problem of large\\nlanguage model-based human-agent collaboration,\\ndelving into the synergy of human intuition and\\nexpertise with the computational prowess of LLM-\\nbased agents, particularly emphasizing their appli-\\ncation in intricate decision-making tasks. We intro-\\nduce a reinforcement learning-based approach for\\nhuman-agent collaboration, named ReHAC. Cen-\\ntral to ReHAC is a learnable policy model designed\\n\\nnarios where high accuracy is crucial, such as the\\nlegal or financial domains. Addressing this chal-\\nlenge extends beyond just enhancing the agents’\\ncapabilities. Incorporating human intuition and\\nwisdom is equally vital for the effective manage-\\nment of these intricate and evolving tasks, offering\\na complementary approach to the limitations of\\ncurrent agent technologies.\\nIn this work, we introduce the problem of LLM-\\nbased human-agent collaboration for complex\\ntask solving , aiming to augment the capabilities of\\nLLM-based agents by integrating human intuition\\nand wisdom. The idea is analogous to the evolu-\\ntion in autonomous driving technology, which has\\nbeen categorized into varying levels of autonomy,\\nranging from no automation, conditional automa-\\ntion to full automation (Khan et al., 2022; SAE\\nInternational, 2021). Referring to this framework,\\nwe define the different levels of human-agent col-\\nlaboration, as illustrated in Figure 1. Applying\\nthis conditional automation mode to LLM-based\\nagents offers a practical path for their deployment\\nin real-world scenarios, acknowledging the current\\nlimitations in their cognitive capabilities. Instead\\nof aiming for full automation, human-agent col-\\nlaboration under the paradigm of conditional au-\\ntomation enables humans to intervene the complex\\ntask-solving when necessary, while agents handle\\nmost of the sub-tasks. This takes advantage of both\\nhuman and machine intelligence.\\nWhile advancements in LLMs significantly en-\\nhance the capacity for mutual understanding in\\nhuman-agent collaboration, several crucial chal-\\nlenges persist. These challenges include defining\\nthe division of labor between humans and agents,\\ndetermining the granularity of tool execution, man-\\naging proactive interruption, and implementing\\nmulti-level intervention. However, our research\\nspecifically focuses on scenarios where humans\\ndirectly replace agents in action. The key chal-\\nlenge we aim to address in human-agent collab-\\noration lies in determining the optimal stages for\\nhuman intervention in task-solving and minimiz-\\ning such intervention to enhance efficiency. Some\\nresearchers have made preliminary attempts, by de-\\nsigning heuristic rules or specialized prompts to\\ndetermine the stages at which agents should seek\\nhuman assistance (Cai et al., 2023; Wu et al., 2022a;\\nMehta et al., 2023; Wang et al., 2023b). However,\\nthese rule-based or prompt-driven approaches are\\nheavily reliant on specific application contexts andlack universality. They often demand a deep under-\\nstanding of the domain and substantial experience\\nfrom the designers, otherwise, suboptimal design\\nchoices can lead to reduced performance. Apart\\nfrom that, a standardized formal framework and\\nuniversally accepted paradigm for leveraging large\\nlanguage models (LLMs) in human-agent collabo-\\nration is still lacking.\\nTo overcome the aforementioned challenges, we\\npropose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC , aimed at\\neffectively combining human intervention with the\\nautomation capabilities of LLM-based agents. Our\\nmethod, leveraging reinforcement learning, trains\\na policy model to dynamically identify the most\\nadvantageous moments for human input during the\\ntask-solving process. ReHAC is a learnable gen-\\neral framework that can be applied to various sce-\\nnarios and does not require additional prior knowl-\\nedge to design rules and prompts. For training\\nthis policy model, we collect a dataset compris-\\ning tasks collaboratively completed by humans and\\nLLM-based agents, utilized for the offline training\\nof the policy model. We conducted extensive ex-\\nperiments on three multi-step reasoning datasets:\\nHotpotQA, StrategyQA, and InterCode, using two\\npopular LLM-based agent frameworks, ReAct and\\n\"Try-again\". The experimental results indicate that\\nwith a policy model learned from limited data, Re-\\nHAC can effectively allocate human intervention\\nin human-agent collaboration scenarios, thereby\\n\\nnarios where high accuracy is crucial, such as the\\nlegal or financial domains. Addressing this chal-\\nlenge extends beyond just enhancing the agents’\\ncapabilities. Incorporating human intuition and\\nwisdom is equally vital for the effective manage-\\nment of these intricate and evolving tasks, offering\\na complementary approach to the limitations of\\ncurrent agent technologies.\\nIn this work, we introduce the problem of LLM-\\nbased human-agent collaboration for complex\\ntask solving , aiming to augment the capabilities of\\nLLM-based agents by integrating human intuition\\nand wisdom. The idea is analogous to the evolu-\\ntion in autonomous driving technology, which has\\nbeen categorized into varying levels of autonomy,\\nranging from no automation, conditional automa-\\ntion to full automation (Khan et al., 2022; SAE\\nInternational, 2021). Referring to this framework,\\nwe define the different levels of human-agent col-\\nlaboration, as illustrated in Figure 1. Applying\\nthis conditional automation mode to LLM-based\\nagents offers a practical path for their deployment\\nin real-world scenarios, acknowledging the current\\nlimitations in their cognitive capabilities. Instead\\nof aiming for full automation, human-agent col-\\nlaboration under the paradigm of conditional au-\\ntomation enables humans to intervene the complex\\ntask-solving when necessary, while agents handle\\nmost of the sub-tasks. This takes advantage of both\\nhuman and machine intelligence.\\nWhile advancements in LLMs significantly en-\\nhance the capacity for mutual understanding in\\nhuman-agent collaboration, several crucial chal-\\nlenges persist. These challenges include defining\\nthe division of labor between humans and agents,\\ndetermining the granularity of tool execution, man-\\naging proactive interruption, and implementing\\nmulti-level intervention. However, our research\\nspecifically focuses on scenarios where humans\\ndirectly replace agents in action. The key chal-\\nlenge we aim to address in human-agent collab-\\noration lies in determining the optimal stages for\\nhuman intervention in task-solving and minimiz-\\ning such intervention to enhance efficiency. Some\\nresearchers have made preliminary attempts, by de-\\nsigning heuristic rules or specialized prompts to\\ndetermine the stages at which agents should seek\\nhuman assistance (Cai et al., 2023; Wu et al., 2022a;\\nMehta et al., 2023; Wang et al., 2023b). However,\\nthese rule-based or prompt-driven approaches are\\nheavily reliant on specific application contexts andlack universality. They often demand a deep under-\\nstanding of the domain and substantial experience\\nfrom the designers, otherwise, suboptimal design\\nchoices can lead to reduced performance. Apart\\nfrom that, a standardized formal framework and\\nuniversally accepted paradigm for leveraging large\\nlanguage models (LLMs) in human-agent collabo-\\nration is still lacking.\\nTo overcome the aforementioned challenges, we\\npropose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC , aimed at\\neffectively combining human intervention with the\\nautomation capabilities of LLM-based agents. Our\\nmethod, leveraging reinforcement learning, trains\\na policy model to dynamically identify the most\\nadvantageous moments for human input during the\\ntask-solving process. ReHAC is a learnable gen-\\neral framework that can be applied to various sce-\\nnarios and does not require additional prior knowl-\\nedge to design rules and prompts. For training\\nthis policy model, we collect a dataset compris-\\ning tasks collaboratively completed by humans and\\nLLM-based agents, utilized for the offline training\\nof the policy model. We conducted extensive ex-\\nperiments on three multi-step reasoning datasets:\\nHotpotQA, StrategyQA, and InterCode, using two\\npopular LLM-based agent frameworks, ReAct and\\n\"Try-again\". The experimental results indicate that\\nwith a policy model learned from limited data, Re-\\nHAC can effectively allocate human intervention\\nin human-agent collaboration scenarios, thereby', name='retrieve_info_from_paper', tool_call_id='call_GgtP493vqXxwhYKfIYkywKyf')]}\n",
      "'\\n---\\n'\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='The authors set up the collaboration between humans and large language models (LLMs) through a reinforcement learning-based approach called ReHAC (Reinforcement Learning-based Human-Agent Collaboration). Here are the key elements of their setup:\\n\\n1. **Reinforcement Learning Framework**: They use a reinforcement learning framework to train a policy model. This model dynamically identifies the most advantageous moments for human input during the task-solving process.\\n\\n2. **Learnable Policy Model**: The policy model is designed to be general and learnable, meaning it can be applied to various scenarios without requiring additional prior knowledge to design rules and prompts.\\n\\n3. **Dataset for Training**: They collect a dataset comprising tasks collaboratively completed by humans and LLM-based agents. This dataset is used for the offline training of the policy model.\\n\\n4. **Experiments and Evaluation**: Extensive experiments are conducted on three multi-step reasoning datasets: HotpotQA, StrategyQA, and InterCode. They use two popular LLM-based agent frameworks, ReAct and \"Try-again,\" to evaluate the effectiveness of ReHAC.\\n\\n5. **Conditional Automation**: The collaboration is set up under the paradigm of conditional automation, where humans intervene in complex task-solving when necessary, while agents handle most of the sub-tasks. This approach leverages both human and machine intelligence.\\n\\n6. **Challenges Addressed**: The method aims to overcome challenges such as defining the division of labor between humans and agents, determining the granularity of tool execution, managing proactive interruption, and implementing multi-level intervention.\\n\\nBy integrating human intuition and expertise with the computational capabilities of LLM-based agents, the authors aim to enhance the efficiency and effectiveness of human-agent collaboration in intricate decision-making tasks.', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, id='run-c238d790-209e-4823-bb57-95d4ba9808ae-0')]}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"In this paper how do the authors set up the collaboration between the human and the LLMs?\"),\n",
    "    ]\n",
    "}\n",
    "for output in compiled_graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ea9ed7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The framework introduced in the paper for setting up collaboration between humans and large language models (LLMs) is called **ReHAC** (Reinforcement Learning-based Human-Agent Collaboration). This method leverages reinforcement learning to dynamically identify the most advantageous moments for human input during the task-solving process.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"What is the name of the framework in this paper that sets up the collaboration between the human and the LLMs?\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "output = compiled_graph.invoke(inputs)\n",
    "\n",
    "output['messages'][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidhya-agents",
   "language": "python",
   "name": "vidhya-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
